{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cdd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== TRAINING CODE ==================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Load dataset\n",
    "cols = ['unit', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + [f'sensor_{i}' for i in range(1, 22)]\n",
    "df = pd.read_csv(\"/content/train_FD001.txt\", sep=' ', header=None)\n",
    "df.drop(columns=[26, 27], inplace=True)\n",
    "df.columns = cols\n",
    "\n",
    "# Calculate RUL\n",
    "rul = df.groupby('unit')['cycle'].max().reset_index()\n",
    "rul.columns = ['unit', 'max']\n",
    "df = df.merge(rul, on='unit')\n",
    "df['RUL'] = df['max'] - df['cycle']\n",
    "df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# Clip and transform RUL\n",
    "df['RUL'] = np.clip(df['RUL'], 0, 125)\n",
    "df['log_RUL'] = np.log1p(df['RUL'])\n",
    "\n",
    "# Features\n",
    "useful_sensors = ['cycle','sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8',\n",
    "                  'sensor_11', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21']\n",
    "features = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + useful_sensors\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Generate sequences\n",
    "def gen_sequence(id_df, seq_len, features):\n",
    "    data_array = id_df[features].values\n",
    "    return np.array([data_array[i:i+seq_len] for i in range(len(data_array) - seq_len)])\n",
    "\n",
    "def gen_labels(id_df, seq_len, label='log_RUL'):\n",
    "    return id_df[label].values[seq_len:]\n",
    "\n",
    "SEQ_LEN = 50\n",
    "seq_array, label_array = [], []\n",
    "\n",
    "for unit in df['unit'].unique():\n",
    "    unit_df = df[df['unit'] == unit]\n",
    "    if len(unit_df) >= SEQ_LEN:\n",
    "        seq_array.extend(gen_sequence(unit_df, SEQ_LEN, features))\n",
    "        label_array.extend(gen_labels(unit_df, SEQ_LEN))\n",
    "\n",
    "X = np.asarray(seq_array)\n",
    "y = np.asarray(label_array)\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Input((SEQ_LEN, X.shape[2])),\n",
    "    LSTM(64, return_sequences=True, kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.4),\n",
    "    LSTM(32, kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.4),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "model_checkpoint = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d29e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(scaler, open(\"scaler.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab235e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== TESTING CODE ==================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "cols = ['unit', 'cycle', 'op_setting_1', 'op_setting_2',\n",
    "        'op_setting_3'] + [f'sensor_{i}' for i in range(1, 22)]\n",
    "test_df = pd.read_csv(\"/content/test_FD001.txt\", sep=' ', header=None)\n",
    "test_df.drop(columns=[26, 27], inplace=True)\n",
    "test_df.columns = cols\n",
    "\n",
    "true_rul = pd.read_csv(\"/content/RUL_FD001.txt\", header=None).values.flatten()\n",
    "\n",
    "# Normalize (same features/scaler)\n",
    "features = ['cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + [\n",
    "    'sensor_2', 'sensor_3', 'sensor_4', 'sensor_7', 'sensor_8',\n",
    "    'sensor_11', 'sensor_15', 'sensor_17', 'sensor_20', 'sensor_21'\n",
    "]\n",
    "train_df = pd.read_csv(\"/content/train_FD001.txt\", sep=' ', header=None)\n",
    "train_df.drop(columns=[26, 27], inplace=True)\n",
    "train_df.columns = cols\n",
    "scaler = MinMaxScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])\n",
    "\n",
    "# Generate sequences\n",
    "\n",
    "\n",
    "def gen_sequence(id_df, seq_len, features):\n",
    "    data_array = id_df[features].values\n",
    "    return np.array([data_array[i:i+seq_len] for i in range(len(data_array) - seq_len + 1)])\n",
    "\n",
    "\n",
    "SEQ_LEN = 50\n",
    "X_test, y_true = [], []\n",
    "\n",
    "for i, unit in enumerate(test_df['unit'].unique()):\n",
    "    unit_df = test_df[test_df['unit'] == unit]\n",
    "    if len(unit_df) >= SEQ_LEN:\n",
    "        sequences = gen_sequence(unit_df, SEQ_LEN, features)\n",
    "        last_sequences = sequences[-5:]  # Take last 5 sequences\n",
    "        X_test.append(np.mean(last_sequences, axis=0))\n",
    "        y_true.append(true_rul[i])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Load model & predict\n",
    "model = load_model(\"best_model.keras\")\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "y_pred = np.clip(np.expm1(y_pred), 0, 125)\n",
    "y_true = np.clip(y_true, 0, 125)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"Test MSE: {mse:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_true, label='True RUL')\n",
    "plt.plot(y_pred, label='Predicted RUL')\n",
    "plt.xlabel('Engine ID')\n",
    "plt.ylabel('Remaining Useful Life')\n",
    "plt.title('True vs Predicted RUL')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faef1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
